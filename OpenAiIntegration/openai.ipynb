{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2, os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def connect_to_database():\n",
    "    try:\n",
    "        # Load environment variables\n",
    "        db_host = os.getenv(\"DB_HOST\")\n",
    "        db_port = os.getenv(\"DB_PORT\")\n",
    "        db_name = os.getenv(\"DB_NAME\")\n",
    "        db_user = os.getenv(\"DB_USER\")\n",
    "        db_password = os.getenv(\"DB_PASSWORD\")\n",
    "        \n",
    "        # Connect to the database\n",
    "        db_connection = psycopg2.connect(\n",
    "            host=db_host,\n",
    "            port=db_port,\n",
    "            database=db_name,\n",
    "            user=db_user,\n",
    "            password=db_password,\n",
    "            cursor_factory=psycopg2.extras.DictCursor\n",
    "        )\n",
    "        return db_connection\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to database: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_articles(db_connection, hours=24):\n",
    "    try:\n",
    "        # Define the time range for the last X hours from the current time\n",
    "        current_time = datetime.now()\n",
    "        time_x_hours_ago = current_time - timedelta(hours=hours)\n",
    "\n",
    "        # Create a cursor object\n",
    "        cursor = db_connection.cursor()\n",
    "\n",
    "        # Execute a SELECT query to retrieve all scraped articles within the specified time range\n",
    "        query = \"SELECT * FROM scraped_articles WHERE publish_date >= %s\"\n",
    "        \n",
    "        # Execute the query\n",
    "        cursor.execute(query, (time_x_hours_ago,))\n",
    "        \n",
    "        # Fetch all the results\n",
    "        all_articles_data = cursor.fetchall()\n",
    "        \n",
    "        # List to store articles\n",
    "        articles = []\n",
    "        \n",
    "        # Extract articles' text\n",
    "        for article_data in all_articles_data:\n",
    "            articles.append(article_data['data'])\n",
    "                \n",
    "        return articles\n",
    "            \n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Close cursor\n",
    "        if cursor is not None:\n",
    "            cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from enum import Enum\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=OPEN_AI_API_KEY)\n",
    "\n",
    "Embedding = Enum(\"Embedding\", [\"text-embedding-ada-002\", \"text-embedding-3-small\", \"text-embedding-3-large\"])\n",
    "\n",
    "def get_embeddings(text: str, model: Embedding):\n",
    "    res = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    return res.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def make_clusters(similarity_matrix, similarity_threshold, articles):\n",
    "    \"\"\"\n",
    "    Groups articles into clusters based on a similarity matrix and a similarity threshold.\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix (ndarray): Matrix representing the pairwise similarities between articles.\n",
    "        similarity_threshold (float): Threshold value for clustering.\n",
    "        articles (list): List of articles\n",
    "\n",
    "    Returns:\n",
    "        list: List of clusters, where each cluster is a list of articles.\n",
    "    \"\"\"\n",
    "    # Initialize the AgglomerativeClustering model with the specified threshold and linkage criterion.\n",
    "    # This setup allows the number of clusters to be determined based on the similarity threshold rather than being predefined.\n",
    "    clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=similarity_threshold, linkage='average')\n",
    "    \n",
    "    # Apply the clustering model to the similarity matrix to identify clusters.\n",
    "    # The fit_predict method returns an array where each element corresponds to the cluster assignment of each article.\n",
    "    clusters = clustering_model.fit_predict(similarity_matrix)\n",
    "\n",
    "    # Initialize a list of empty lists to hold the articles for each cluster.\n",
    "    # The number of clusters is determined by the unique cluster identifiers in the 'clusters' array.\n",
    "    clustered_articles = [[] for _ in range(len(set(clusters)))]\n",
    "    \n",
    "    # Iterate over each article's group the articles accordingly.\n",
    "    for i in range(len(articles)):\n",
    "        # Append each article to its corresponding cluster based on the cluster assignment.\n",
    "        # The 'articles[i]' part assumes there's an 'articles' list available in the scope where each article's index corresponds to its position in the similarity matrix.\n",
    "        clustered_articles[clusters[i]].append(articles[i])\n",
    "\n",
    "    # Return the list of article clusters, where each cluster is represented as a list of articles.\n",
    "    return clustered_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_cluster(cluster):\n",
    "    # combine all articles in cluster into one string\n",
    "    # so that we can summarize them together using the OpenAI API\n",
    "    cluster_string = \" \".join(cluster)\n",
    "    # get the number of tokens in the cluster\n",
    "    num_tokens = num_tokens_from_string(cluster_string, \"cl100k_base\")\n",
    "    # summarize the cluster\n",
    "    summary = client.chat.completions.create(\n",
    "        # model=\"gpt-3.5-turbo\",\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": '''   Please summarize the following Hebrew article,\n",
    "                                                focusing on the main points, arguments, and conclusions.\n",
    "                                                Highlight any significant data, quotes, or statistics mentioned,\n",
    "                                                and note the context in which they are presented. \n",
    "                                                If the article discusses multiple perspectives or debates,\n",
    "                                                please outline these distinctly. Additionally,\n",
    "                                                if there are any implications or recommendations made by the author,\n",
    "                                                include these in the summary.\n",
    "                                                Finally, if the article references specific events, individuals, or sources, \n",
    "                                                please identify these and their relevance to the article's overall message.\n",
    "                                                pay attention to correct syntax and grammar in Hebrew'''},\n",
    "            {\"role\": \"user\", \"content\": f\"Article: {cluster_string}\"},\n",
    "        ],\n",
    "        max_tokens=int(num_tokens * 0.1),\n",
    "        # response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Develop from here...\n",
    "################################################################\n",
    "################################################################\n",
    "\n",
    "def insert_summarized_article(db_connection, title, article, category):\n",
    "    try:\n",
    "        # Create a cursor object\n",
    "        cursor = db_connection.cursor()\n",
    "        \n",
    "        # SQL query to insert the summarized article into the merged_articles table\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO merged_articles (title, article, category) \n",
    "        VALUES (%s, %s, %s)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query with the provided title, text, and category\n",
    "        cursor.execute(insert_query, (title, article, category))\n",
    "        \n",
    "        # Commit the changes to the database\n",
    "        db_connection.commit()\n",
    "        print(\"Article inserted successfully into merged_articles table.\")\n",
    "        \n",
    "    except psycopg2.Error as e:\n",
    "        # Rollback in case of any error\n",
    "        db_connection.rollback()\n",
    "        print(f\"Error inserting article into database: {e}\")\n",
    "    finally:\n",
    "        # Close the cursor\n",
    "        if cursor is not None:\n",
    "            cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "db_connection = connect_to_database()\n",
    "\n",
    "if db_connection:\n",
    "    # Fetch articles by category\n",
    "    articles = fetch_articles(db_connection)\n",
    "    \n",
    "    for article in articles:\n",
    "        print(article)\n",
    "\n",
    "    # Close database connection\n",
    "    db_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "for model in Embedding:\n",
    "    embeddings = [get_embeddings(article, model.name) for article in articles]\n",
    "    similarities = cosine_similarity(embeddings)\n",
    "    print(f\"-----Model: {model.name}-----\")\n",
    "    print(similarities.round(3))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = make_clusters(similarities, 0.6, articles)\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(f\"-----Cluster {i}-----\")\n",
    "    for article in cluster:\n",
    "        print(article)\n",
    "        print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each cluster, combine the articles into one string and summarize it using the summarizer function\n",
    "summaries = [summarize_cluster(cluster) for cluster in clusters]\n",
    "for i, summary in enumerate(summaries):\n",
    "    print(f\"-----Cluster {i}-----\")\n",
    "    for line in summary.choices[0].message.content.split(\".\"):\n",
    "        print(line)\n",
    "        print(\"\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
